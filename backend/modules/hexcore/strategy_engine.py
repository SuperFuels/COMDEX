"""
ðŸŽ¯ StrategyEngine - Resonant Strategic Reasoning Core
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Phase 55 upgrade:
Integrates Î˜ resonance, context entropy, and ethics metrics into
planning evaluation and optimization.

Features:
  * Î˜ heartbeat coupling -> reasoning temperature modulation
  * RMC coherence feedback -> adaptive weighting
  * Ethics influence -> bias correction for moral alignment
  * Plan coherence scoring -> symbolic + semantic evaluation
"""
import time
import math
import random
import json
from datetime import datetime
from pathlib import Path

# âœ… DNA switch for upgrade tracking
from backend.modules.dna_chain.switchboard import DNA_SWITCH
DNA_SWITCH.register(__file__)

# âš› Resonance + ethics subsystems
from backend.modules.aion_resonance.resonance_heartbeat import ResonanceHeartbeat
from backend.modules.aion_language.resonant_memory_cache import ResonantMemoryCache

# Optional ethics weighting (safe import)
try:
    from backend.modules.ethics.ethics_engine import EthicsEngine
except Exception:
    EthicsEngine = None

STATE_FILE = Path("data/analysis/strategy_engine_state.json")
STATE_FILE.parent.mkdir(parents=True, exist_ok=True)


class StrategyEngine:
    def __init__(self):
        self.temperature = 0.5              # reasoning entropy temperature (0-1)
        self.last_eval = None
        self.last_resonance = 0.7
        self.last_ethics = 1.0
        self.rmc = ResonantMemoryCache()
        self.Î˜ = ResonanceHeartbeat(namespace="strategy_engine")
        self.ethics = EthicsEngine() if EthicsEngine else None
        self.Î˜.register_listener(self._on_heartbeat)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ðŸ”„ Resonance coupling
    def _on_heartbeat(self, metrics):
        """Dynamic adjustment from Î˜ feedback."""
        drift = metrics.get("resonance_drift", 0.0)
        sqi = metrics.get("sqi", 0.7)
        stability = metrics.get("stability", 0.6)
        # Adjust reasoning temperature using entropy drift
        delta_T = (0.5 - stability) * 0.1 + drift * 0.05
        self.temperature = max(0.1, min(1.0, self.temperature + delta_T))
        self.last_resonance = sqi
        self._save_state()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ðŸ§­ Evaluation logic
    def evaluate(self, plan):
        """
        Evaluate a plan's symbolic and harmonic viability.
        Uses resonance, ethics, and entropy-adjusted scoring.
        """
        goal = plan.get("goal", "undefined")
        base_score = self._semantic_alignment(goal)
        resonance_score = self.rmc.get("last_exported_strategy", {}).get("count", 1) / 2500
        resonance_score = min(1.0, resonance_score)

        ethics_weight = 1.0
        if self.ethics:
            ethics_weight = self.ethics.evaluate_alignment(goal)
        ethics_weight = max(0.3, min(1.2, ethics_weight))

        temp_factor = 1 - abs(self.temperature - 0.5)
        final_score = (base_score * 0.5 + resonance_score * 0.3 + temp_factor * 0.2) * ethics_weight
        final_score = round(final_score, 3)
        self.last_eval = final_score

        self._log_evaluation(goal, final_score, base_score, resonance_score, ethics_weight)
        return final_score

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _semantic_alignment(self, goal: str):
        """Approximate symbolic coherence via cached resonance or lexical mean."""
        cache = self.rmc.lookup(goal)
        if cache and "sqi" in cache:
            return cache["sqi"]
        return round(random.uniform(0.4, 0.85), 3)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def adjust_temperature(self, delta: float):
        self.temperature = max(0.1, min(1.0, self.temperature + delta))
        self._save_state()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def choose_best(self, plans):
        """
        Evaluate a list of plans and choose the highest-resonance candidate.
        """
        if not plans:
            return None
        scored = [(p, self.evaluate(p)) for p in plans]
        best_plan, best_score = max(scored, key=lambda x: x[1])
        print(f"[ðŸŽ¯] Best plan selected -> {best_plan.get('goal')} (score={best_score:.3f})")
        return best_plan

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _log_evaluation(self, goal, final, base, res, ethics):
        entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "goal": goal,
            "base": base,
            "resonance": res,
            "ethics": ethics,
            "temperature": round(self.temperature, 3),
            "final": final
        }
        STATE_FILE.write_text(json.dumps(entry, indent=2))

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _save_state(self):
        state = {
            "temperature": round(self.temperature, 3),
            "last_eval": self.last_eval,
            "last_resonance": self.last_resonance,
            "last_ethics": self.last_ethics
        }
        with open(STATE_FILE, "w") as f:
            json.dump(state, f, indent=2)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_state(self):
        return {
            "temperature": self.temperature,
            "last_eval": self.last_eval,
            "last_resonance": self.last_resonance,
            "last_ethics": self.last_ethics
        }

    def execute_plan(self, plan: dict):
        """Executes a symbolic plan (stub until full strategy logic is linked)."""
        goal = plan.get("goal", "undefined")
        print(f"[StrategyEngine] ðŸ§­ Executing plan goal='{goal}' ...")
        # placeholder for real execution logic
        time.sleep(0.3)
        result = {"goal": goal, "status": "executed", "timestamp": time.time()}
        return result

# ðŸ§ª Local diagnostic run
if __name__ == "__main__":
    engine = StrategyEngine()
    plans = [
        {"goal": "optimize harmonic resonance"},
        {"goal": "increase stability of awareness feedback"},
        {"goal": "enhance moral alignment"}
    ]
    engine.choose_best(plans)
    print(engine.get_state())